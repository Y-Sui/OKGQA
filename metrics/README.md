## Open-ended Question Answering with Knowledge Graph

This folder holds metrics for assessing the quality of LLM generation for the OKG benchmark. OKG is an open-ended QA benchmark that encompasses synthesized queries and retrieved 2-hop sub-graphs from DBpedia. Given the complexities inherent in the open-ended QA setting, collecting and annotating ground-truth answers pose significant challenges. To evaluate the quality of answers generated by language models, an LLM-based metric is utilized for automatic verification. This metric assists in assessing the accuracy and relevance of the responses produced by the language models, compensating for the absence of labeled answers. This methodology effectively addresses the challenges of annotating extensive and diverse answers typical of open-ended queries.

The metrics are defined as follows:

- **HallucinationMetric**: in the context of generative model, hallucinations refers to the generation or claims that are not supported by the provided retrieval context or source material. The severity of hallucination is measured by the faithfulness score, which ranges from 0 to 1, with a higher score indicating better alignment with the source material, while contradictions and unsupported claims often directly leads to a lower faithfulness score.
   - $$\text{score} = \frac{\text{Number of Truthful Claims}}{\text{Total Number of Claims}}$$
   - The metric first uses an LLM to extract all claims made in model's generation and then classify whether each claim is trustworthy based on the facts from retrieved context or source material.
